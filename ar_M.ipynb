{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Haavue-S8bz2",
        "outputId": "ab0a2a46-96b5-4264-c18c-f2fc311b0765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-9af04d04d6b1>:24: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sample_rate = librosa.load(row['Path'], sr=None)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive  # Import the Google Drive connection module\n",
        "import zipfile  # Module for handling zip files\n",
        "import os  # Operating system module\n",
        "import pandas as pd  # Library for data manipulation and analysis\n",
        "import numpy as np  # Numerical computing library\n",
        "import librosa  # Audio processing library\n",
        "from sklearn.model_selection import train_test_split  # Splitting data into training and testing sets\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Dropout, Reshape\n",
        "from tensorflow.keras.models import Model, Sequential  # Keras model classes\n",
        "from tensorflow.keras.optimizers import Adam  # Adam optimizer\n",
        "from tensorflow.keras.utils import to_categorical  # Utility function for one-hot encoding\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # Callback for early stopping during training\n",
        "from tensorflow.image import resize  # Image resizing function\n",
        "\n",
        "# Load and preprocess audio data\n",
        "def load_and_preprocess_data(data_excel, target_shape=(128, 128)):\n",
        "    data = []  # List to store preprocessed audio data\n",
        "    labels = []  # List to store labels\n",
        "\n",
        "    # Iterate through rows in the metadata file\n",
        "    for i, row in data_excel.iterrows():\n",
        "        # Load audio data using librosa\n",
        "        audio_data, sample_rate = librosa.load(row['Path'], sr=None)\n",
        "\n",
        "        # Extract Mel spectrogram and resize it\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
        "        mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
        "\n",
        "        # Append data and labels\n",
        "        data.append(mel_spectrogram)\n",
        "        labels.append(row['Pronunciation'])\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Connect to Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/ASMDD.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# Read metadata file\n",
        "metadata_file = '/content/ASMDD.csv'\n",
        "df = pd.read_csv(metadata_file)\n",
        "\n",
        "# Adjust Path to add Colab path\n",
        "df['Path'] = '/content/' + df['Path']\n",
        "\n",
        "# Import Reshape layer\n",
        "from tensorflow.keras.layers import Reshape\n",
        "\n",
        "# Replace string labels with numeric values\n",
        "mapping = {'Right': 1, 'Wrong': 0}\n",
        "df['Pronunciation'] = df['Pronunciation'].replace(mapping)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "data, labels = load_and_preprocess_data(df)\n",
        "labels = to_categorical(labels, num_classes=2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a neural network model with Long short-term memory (LSTM)\n",
        "time_steps, frequency_bins = X_train[0].shape[0], X_train[0].shape[1]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDRF5tD1k_ea",
        "outputId": "03ddaa18-b5fb-46e6-daef-6ab84ce694c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "271/271 [==============================] - 19s 49ms/step - loss: 0.4321 - accuracy: 0.8474 - val_loss: 0.4488 - val_accuracy: 0.8437 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "271/271 [==============================] - 12s 44ms/step - loss: 0.4207 - accuracy: 0.8527 - val_loss: 0.4315 - val_accuracy: 0.8409 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.4100 - accuracy: 0.8522 - val_loss: 0.4356 - val_accuracy: 0.8400 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.4121 - accuracy: 0.8504 - val_loss: 0.4234 - val_accuracy: 0.8409 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.4107 - accuracy: 0.8501 - val_loss: 0.4319 - val_accuracy: 0.8427 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "271/271 [==============================] - 12s 44ms/step - loss: 0.4087 - accuracy: 0.8529 - val_loss: 0.4261 - val_accuracy: 0.8446 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.4100 - accuracy: 0.8513 - val_loss: 0.4241 - val_accuracy: 0.8418 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "271/271 [==============================] - 12s 46ms/step - loss: 0.4067 - accuracy: 0.8529 - val_loss: 0.4325 - val_accuracy: 0.8437 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "271/271 [==============================] - 12s 46ms/step - loss: 0.4013 - accuracy: 0.8527 - val_loss: 0.4277 - val_accuracy: 0.8427 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.4018 - accuracy: 0.8538 - val_loss: 0.4275 - val_accuracy: 0.8418 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "271/271 [==============================] - 12s 44ms/step - loss: 0.3991 - accuracy: 0.8529 - val_loss: 0.4252 - val_accuracy: 0.8409 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.3965 - accuracy: 0.8545 - val_loss: 0.4404 - val_accuracy: 0.8409 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.4004 - accuracy: 0.8545 - val_loss: 0.4323 - val_accuracy: 0.8427 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.3995 - accuracy: 0.8541 - val_loss: 0.4310 - val_accuracy: 0.8437 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.3983 - accuracy: 0.8543 - val_loss: 0.4288 - val_accuracy: 0.8418 - lr: 2.5000e-05\n",
            "Epoch 16/100\n",
            "271/271 [==============================] - 12s 44ms/step - loss: 0.3895 - accuracy: 0.8566 - val_loss: 0.4363 - val_accuracy: 0.8427 - lr: 2.5000e-05\n",
            "Epoch 17/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.3884 - accuracy: 0.8552 - val_loss: 0.4300 - val_accuracy: 0.8437 - lr: 2.5000e-05\n",
            "Epoch 18/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.3879 - accuracy: 0.8562 - val_loss: 0.4422 - val_accuracy: 0.8446 - lr: 2.5000e-05\n",
            "Epoch 19/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.3876 - accuracy: 0.8559 - val_loss: 0.4318 - val_accuracy: 0.8418 - lr: 2.5000e-05\n",
            "Epoch 20/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.3869 - accuracy: 0.8568 - val_loss: 0.4340 - val_accuracy: 0.8381 - lr: 1.2500e-05\n",
            "Epoch 21/100\n",
            "271/271 [==============================] - 12s 45ms/step - loss: 0.3852 - accuracy: 0.8578 - val_loss: 0.4375 - val_accuracy: 0.8400 - lr: 1.2500e-05\n",
            "34/34 [==============================] - 1s 24ms/step - loss: 0.4261 - accuracy: 0.8446\n",
            "Test Accuracy: 0.8445883393287659\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Define learning rate scheduler with adjusted parameters\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "# Define model architecture with batch normalization\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(time_steps, frequency_bins, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Reshape((-1, frequency_bins * 2)))\n",
        "\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(256))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with learning rate scheduler\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=16,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping, lr_scheduler])\n",
        "\n",
        "# Save the model\n",
        "model.save('audio_model_optimized_v4.h5')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa  # Library for audio processing\n",
        "import pandas as pd  # Data manipulation and analysis\n",
        "import numpy as np  # Numerical operations\n",
        "from tensorflow.keras.models import Sequential  # For linear stacking of layers\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input, Bidirectional, LSTM, Reshape\n",
        "from tensorflow.keras.optimizers import Adam  # Optimization method\n",
        "from tensorflow.keras.utils import to_categorical  # Convert labels to one-hot vectors\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Callbacks for training control\n",
        "from sklearn.model_selection import train_test_split  # Splitting data into training and testing\n",
        "from tensorflow.keras.regularizers import l2  # Regularization\n",
        "from sklearn.preprocessing import StandardScaler  # Standardization of datasets\n",
        "from google.colab import drive  # Import the Google Drive connection module\n",
        "import zipfile  # Module for handling zip files\n",
        "import os  # Operating system module\n",
        "\n",
        "def load_and_preprocess_data(data_excel, target_shape=(96, 64)):\n",
        "    data = []\n",
        "    labels = []\n",
        "    # Iterate through each row in the dataframe\n",
        "    for i, row in data_excel.iterrows():\n",
        "        # Load the audio file, ensuring a consistent sampling rate\n",
        "        audio_data, sample_rate = librosa.load(row['Path'], sr=44100)\n",
        "        # Compute mel spectrogram\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_fft=2048, hop_length=512, n_mels=target_shape[0])\n",
        "        # Convert power spectrogram to decibel units\n",
        "        mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
        "        # Resize the mel spectrogram to fit the model input\n",
        "        mel_spectrogram = np.resize(mel_spectrogram, target_shape)\n",
        "        # Append the preprocessed spectrogram and label to the lists\n",
        "        data.append(mel_spectrogram)\n",
        "        labels.append(row['Pronunciation'])\n",
        "    # Convert lists to numpy arrays for processing in machine learning models\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "\n",
        "# Connect to Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/ASMDD.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "metadata_file = '/content/ASMDD.csv'\n",
        "df = pd.read_csv(metadata_file)\n",
        "df['Path'] = '/content/' + df['Path']\n",
        "mapping = {'Right': 1, 'Wrong': 0}\n",
        "df['Pronunciation'] = df['Pronunciation'].replace(mapping)\n",
        "\n",
        "data, labels = load_and_preprocess_data(df)\n",
        "labels = to_categorical(labels, num_classes=2)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(data, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(96, 64, 1)),  # Input layer specifying input shape\n",
        "    Conv2D(32, (3, 3), activation='relu'),  # 2D Convolutional layer\n",
        "    BatchNormalization(),  # Normalize the activations of the previous layer\n",
        "    MaxPooling2D((2, 2)),  # Max Pooling to reduce spatial dimensions\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Reshape((15, 512)),  # Reshape output for LSTM layers\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),  # Bidirectional LSTM\n",
        "    Bidirectional(LSTM(128)),\n",
        "    Dropout(0.5),  # Dropout for regularization\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')  # Output layer with softmax activation\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=22, batch_size=16)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MX9OUjOSnh_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1f433c-e909-4662-e1f2-8316fe681b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-f450a453fa54>:22: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sample_rate = librosa.load(row['Path'], sr=44100)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/22\n",
            "237/237 [==============================] - 12s 12ms/step - loss: 0.4454 - accuracy: 0.8451\n",
            "Epoch 2/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.4294 - accuracy: 0.8483\n",
            "Epoch 3/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.4267 - accuracy: 0.8491\n",
            "Epoch 4/22\n",
            "237/237 [==============================] - 4s 16ms/step - loss: 0.4185 - accuracy: 0.8488\n",
            "Epoch 5/22\n",
            "237/237 [==============================] - 3s 13ms/step - loss: 0.4039 - accuracy: 0.8485\n",
            "Epoch 6/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.4081 - accuracy: 0.8501\n",
            "Epoch 7/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.3977 - accuracy: 0.8501\n",
            "Epoch 8/22\n",
            "237/237 [==============================] - 3s 13ms/step - loss: 0.3919 - accuracy: 0.8480\n",
            "Epoch 9/22\n",
            "237/237 [==============================] - 4s 16ms/step - loss: 0.3849 - accuracy: 0.8485\n",
            "Epoch 10/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.3726 - accuracy: 0.8491\n",
            "Epoch 11/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.3589 - accuracy: 0.8509\n",
            "Epoch 12/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.3508 - accuracy: 0.8543\n",
            "Epoch 13/22\n",
            "237/237 [==============================] - 3s 14ms/step - loss: 0.3257 - accuracy: 0.8639\n",
            "Epoch 14/22\n",
            "237/237 [==============================] - 3s 14ms/step - loss: 0.3094 - accuracy: 0.8718\n",
            "Epoch 15/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.2672 - accuracy: 0.8932\n",
            "Epoch 16/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.2379 - accuracy: 0.9046\n",
            "Epoch 17/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.1976 - accuracy: 0.9215\n",
            "Epoch 18/22\n",
            "237/237 [==============================] - 4s 17ms/step - loss: 0.1499 - accuracy: 0.9366\n",
            "Epoch 19/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.1318 - accuracy: 0.9471\n",
            "Epoch 20/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.0956 - accuracy: 0.9630\n",
            "Epoch 21/22\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.0784 - accuracy: 0.9738\n",
            "Epoch 22/22\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.0784 - accuracy: 0.9712\n",
            "51/51 [==============================] - 2s 8ms/step - loss: 0.8440 - accuracy: 0.8009\n",
            "Test Accuracy: 0.8008631467819214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nwkqqncef2gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X1t1dBs1nMnj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}